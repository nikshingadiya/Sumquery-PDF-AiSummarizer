{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "import openai\n",
    "import langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-experimental\n",
    "# !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Warren Buffett\"\n",
    "raw_documents = WikipediaLoader(query=query).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = [Document(page_content=t.page_content) for t in raw_documents[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\",openai_api_key=OPENAI_API_KEY,verbose=True)\n",
    "from langchain.llms import KoboldApiLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = KoboldApiLLM(endpoint=\"https://dc01-35-232-216-184.ngrok-free.app\", max_length=100,temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(\"### why most of the people is sad?\\n### Response:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#+BEGIN_HTML      <p>Mostly because they are poor. But this can be solved by making them\n",
      "rich.</P></BR><br/>     </div id=\"answer\"> </DIV > < DIV class = \"answers\" style= 'display : none'>\n",
      "<!-- if you want to hide all answers, use display:\"none\"; -->           This one's for ya! You're so\n",
      "cool.<span data-id='2'>&nbsp;</\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "wrapped_text = textwrap.fill(response, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#+BEGIN_HTML\\n\\n    <p>Mostly because they are poor. But this can be solved by making them rich.</P></BR><br/> \\n   </div id=\"answer\"> </DIV > < DIV class = \"answers\" style= \\'display : none\\'>  <!-- if you want to hide all answers, use display:\"none\"; --> \\n\\n        This one\\'s for ya! You\\'re so cool.<span data-id=\\'2\\'>&nbsp;</'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "507 Server Error: Insufficient Storage for url: https://dc01-35-232-216-184.ngrok-free.app/api/v1/generate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/nikhil/Documents/Github/Sumquery-PDF-AiSummarizer/trash/KoboldAI.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nikhil/Documents/Github/Sumquery-PDF-AiSummarizer/trash/KoboldAI.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Function to generate summary using OpenAI model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nikhil/Documents/Github/Sumquery-PDF-AiSummarizer/trash/KoboldAI.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m chain \u001b[39m=\u001b[39m load_summarize_chain(llm, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m, prompt\u001b[39m=\u001b[39mBULLET_POINT_PROMPT)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nikhil/Documents/Github/Sumquery-PDF-AiSummarizer/trash/KoboldAI.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m summarized_docs \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49mrun(docs)\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/base.py:505\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    506\u001b[0m         _output_key\n\u001b[1;32m    507\u001b[0m     ]\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    511\u001b[0m         _output_key\n\u001b[1;32m    512\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/combine_documents/base.py:122\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    121\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m--> 122\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m    123\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    125\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/combine_documents/stuff.py:171\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs), {}\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/llm.py:298\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    284\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/llm.py:108\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    105\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m    106\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 108\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/chains/llm.py:120\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    118\u001b[0m callbacks \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    121\u001b[0m         prompts,\n\u001b[1;32m    122\u001b[0m         stop,\n\u001b[1;32m    123\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    124\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mbind(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs)\u001b[39m.\u001b[39mbatch(\n\u001b[1;32m    128\u001b[0m         cast(List, prompts), {\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks}\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/llms/base.py:507\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    500\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    501\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    505\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    506\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 507\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    532\u001b[0m                 prompts,\n\u001b[1;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/llms/base.py:1053\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m   1052\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1053\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1054\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1055\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m   1058\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/langchain/llms/koboldai.py:179\u001b[0m, in \u001b[0;36mKoboldApiLLM._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     data[\u001b[39m\"\u001b[39m\u001b[39mstop_sequence\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop\n\u001b[1;32m    175\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    176\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mclean_url(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint)\u001b[39m}\u001b[39;00m\u001b[39m/api/v1/generate\u001b[39m\u001b[39m\"\u001b[39m, json\u001b[39m=\u001b[39mdata\n\u001b[1;32m    177\u001b[0m )\n\u001b[0;32m--> 179\u001b[0m response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    180\u001b[0m json_response \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    183\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m json_response\n\u001b[1;32m    184\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(json_response[\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    185\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m json_response[\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    186\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/Github/Sumquery-PDF-AiSummarizer/venv/lib/python3.8/site-packages/requests/models.py:953\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m     http_error_msg \u001b[39m=\u001b[39m \u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code, reason, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl)\n\u001b[1;32m    952\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m--> 953\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 507 Server Error: Insufficient Storage for url: https://dc01-35-232-216-184.ngrok-free.app/api/v1/generate"
     ]
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "prompt_template =  \"\"\"\n",
    "Task 1: Please write the common person's name.\n",
    "\n",
    "Task 2: Provide a concise 10 bullet point summary of the following:\n",
    "{text}\n",
    "\n",
    "\n",
    "\n",
    "Task 3: Analyze the person's skills and provide a rating on a scale of 1 to 10 for each skill. Use the format below:\n",
    "- Economics: 3/10\n",
    "- Mathematics: 8/10\n",
    "- [Add more skills as needed]\n",
    "\n",
    "Please also return result of Person Name : [Task 1], Summary : [Task 2], Skills: [Task 3]\n",
    "Don't Repeat Info\n",
    "Print Verbose\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# Function to generate summary using OpenAI model\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=BULLET_POINT_PROMPT)\n",
    "summarized_docs = chain.run(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Do you want to add this name to your address book?\"\\nYou can also use the following shortcut keys:\\nCtrl + F2 - Find and replace text\\nCtrl + F3 - Search for next occurrence of text\\nCtrl + F4 - Search for previous occurrence of text\\nCtrl + Shift + N - Open address book\\nCtrl + Shift + O - Open phone book\\nCtrl + Shift + P - Print address book\\nCtrl + Shift + Q - Quit\\n\\nI\\'m going to make this short and sweet because I know everyone is busy getting ready for the holidays. But first, let me just say that I am so grateful for all of you! You\\'ve been so supportive of my work and I really appreciate it. It means so much to me that you like what I do. So thank you. Thank you. Thank you.\\nSo, now that we\\'re done with that...\\nAs you may or may not know, I\\'m a big fan of the Christmas movie Home Alone. My favorite part is when Kevin gets a bunch of toys for Christmas and they\\'re all wrapped up in bright, shiny paper. Then he starts ripping them open and every single toy has a bow on it.\\nHe keeps thinking that the bows are supposed to be there, but they aren\\'t. They\\'re just part of the wrapping. He\\'s so confused by this that he thinks he\\'s missing something. And then he finally realizes that the bows were only meant to be decorative.\\nThat\\'s how I feel about gift wrap. Wrapping gifts is fun and festive, but it doesn\\'t have to be perfect. Don\\'t get hung up on making everything look pretty. Just enjoy the process and have fun with it.\\nThis year, I decided to go with a color scheme of red, white, and blue. It\\'s so classic and I love the way it looks. I used a lot of different patterns and textures to keep things interesting.\\nThe tags are one of my favorite parts. I cut out strips of red, white, and blue felt and hot glued them onto twine. I sewed the ends together and attached a loop to the top. Then I added a piece of ribbon to the back.\\nThese tags would also be cute tied to a package of cookies or some other baked good.\\nFor the packages, I wanted to create a simple design that could be used for any occasion. For example, I made a couple of boxes in a green and silver color scheme.\\nI love the way these turned out. The little star is such a cute accent.\\nAnd here\\'s another box using the same colors.\\nIf you want to see more of my gift wrapping ideas, check out my Gift Wrap Pinterest board.\\nI hope you enjoyed this post and that it inspires you to go a little bit simpler this holiday season. Have a wonderful day!\\n\\nWhen you think of the term “Superhero” what comes to mind? Is it the super strength and speed of Superman? Or maybe the flying abilities of Wonder Woman? Or perhaps the power of Batman? There are many superheroes who have incredible powers, but I believe that none compare to the most powerful superhero of all. His name is Jesus Christ.\\nJesus is the ultimate Superhero. He is God in human flesh. He was born into this world to save us from our sins. In order to understand why Jesus is the greatest superhero, we must first understand what sin is. Sin is anything that separates us from God. It is an offense against Him. When Adam and Eve disobeyed God, they broke a covenant with Him. This caused separation between man and God. Since then, man has tried to find ways to bridge the gap between himself and God. But no matter how hard we try, we will never be able to bridge that gap. We can never earn our way back to God. That is why Jesus came. He bridged the gap between man and God by dying on the cross. Because of this sacrifice, we are now able to have a relationship with God.\\nIt is important to realize that Jesus is not just a superhero; He is also our Savior. Our Savior is someone who saves us from our sins. Without a Savior, we would be lost forever. No one can save us except for Jesus. If you do not know Jesus as your Savior, please pray this prayer:\\nDear Heavenly Father, I am a sinner. I don’t deserve to live in Heaven with You. Please forgive me and come into my heart today. Thank you for saving me. Amen.\\nIf you prayed this prayer, congratulations! You are now saved! If you have any questions about becoming a Christian or if you need help in your new walk with Christ, please contact me at [email protected]\\n\\nIn the past few months, there has been a surge of interest in the study of “neuroplastic'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Do you want to add this name to your address book?\" You can also use the following shortcut keys:\n",
      "Ctrl + F2 - Find and replace text Ctrl + F3 - Search for next occurrence of text Ctrl + F4 - Search\n",
      "for previous occurrence of text Ctrl + Shift + N - Open address book Ctrl + Shift + O - Open phone\n",
      "book Ctrl + Shift + P - Print address book Ctrl + Shift + Q - Quit  I'm going to make this short and\n",
      "sweet because I know everyone is busy getting ready for the holidays. But first, let me just say\n",
      "that I am so grateful for all of you! You've been so supportive of my work and I really appreciate\n",
      "it. It means so much to me that you like what I do. So thank you. Thank you. Thank you. So, now that\n",
      "we're done with that... As you may or may not know, I'm a big fan of the Christmas movie Home Alone.\n",
      "My favorite part is when Kevin gets a bunch of toys for Christmas and they're all wrapped up in\n",
      "bright, shiny paper. Then he starts ripping them open and every single toy has a bow on it. He keeps\n",
      "thinking that the bows are supposed to be there, but they aren't. They're just part of the wrapping.\n",
      "He's so confused by this that he thinks he's missing something. And then he finally realizes that\n",
      "the bows were only meant to be decorative. That's how I feel about gift wrap. Wrapping gifts is fun\n",
      "and festive, but it doesn't have to be perfect. Don't get hung up on making everything look pretty.\n",
      "Just enjoy the process and have fun with it. This year, I decided to go with a color scheme of red,\n",
      "white, and blue. It's so classic and I love the way it looks. I used a lot of different patterns and\n",
      "textures to keep things interesting. The tags are one of my favorite parts. I cut out strips of red,\n",
      "white, and blue felt and hot glued them onto twine. I sewed the ends together and attached a loop to\n",
      "the top. Then I added a piece of ribbon to the back. These tags would also be cute tied to a package\n",
      "of cookies or some other baked good. For the packages, I wanted to create a simple design that could\n",
      "be used for any occasion. For example, I made a couple of boxes in a green and silver color scheme.\n",
      "I love the way these turned out. The little star is such a cute accent. And here's another box using\n",
      "the same colors. If you want to see more of my gift wrapping ideas, check out my Gift Wrap Pinterest\n",
      "board. I hope you enjoyed this post and that it inspires you to go a little bit simpler this holiday\n",
      "season. Have a wonderful day!  When you think of the term “Superhero” what comes to mind? Is it the\n",
      "super strength and speed of Superman? Or maybe the flying abilities of Wonder Woman? Or perhaps the\n",
      "power of Batman? There are many superheroes who have incredible powers, but I believe that none\n",
      "compare to the most powerful superhero of all. His name is Jesus Christ. Jesus is the ultimate\n",
      "Superhero. He is God in human flesh. He was born into this world to save us from our sins. In order\n",
      "to understand why Jesus is the greatest superhero, we must first understand what sin is. Sin is\n",
      "anything that separates us from God. It is an offense against Him. When Adam and Eve disobeyed God,\n",
      "they broke a covenant with Him. This caused separation between man and God. Since then, man has\n",
      "tried to find ways to bridge the gap between himself and God. But no matter how hard we try, we will\n",
      "never be able to bridge that gap. We can never earn our way back to God. That is why Jesus came. He\n",
      "bridged the gap between man and God by dying on the cross. Because of this sacrifice, we are now\n",
      "able to have a relationship with God. It is important to realize that Jesus is not just a superhero;\n",
      "He is also our Savior. Our Savior is someone who saves us from our sins. Without a Savior, we would\n",
      "be lost forever. No one can save us except for Jesus. If you do not know Jesus as your Savior,\n",
      "please pray this prayer: Dear Heavenly Father, I am a sinner. I don’t deserve to live in Heaven with\n",
      "You. Please forgive me and come into my heart today. Thank you for saving me. Amen. If you prayed\n",
      "this prayer, congratulations! You are now saved! If you have any questions about becoming a\n",
      "Christian or if you need help in your new walk with Christ, please contact me at [email protected]\n",
      "In the past few months, there has been a surge of interest in the study of “neuroplastic\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "wrapped_text = textwrap.fill(summarized_docs, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Warren Edward Buffett:\\n- Economics: 10/10\\n- Mathematics: 9/10\\n- Business: 10/10\\n- Investing: 10/10\\n- Philanthropy: 10/10'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
